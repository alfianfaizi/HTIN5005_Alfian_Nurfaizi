# -*- coding: utf-8 -*-
"""Interpretability Metrics

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b2e2BfJsi2YFXizNdmnwc2BWwv-Sffd0
"""

import numpy as np
from sklearn.metrics import accuracy_score, roc_auc_score

def calculate_interpretability(k, n):
    """
    Calculates the Interpretability (I) score.

    Interpretability is defined based on the number of features used (k)
    relative to the total number of features available before selection (n).
    I = 1 indicates maximum interpretability (1 feature used).
    I = 0 indicates minimum interpretability (all features used).

    Args:
        k (int): The number of features used by the final model.
                 Must be >= 1 and <= n.
        n (int): The total number of features available before feature selection.
                 Must be >= 1.

    Returns:
        float: The Interpretability score (between 0 and 1).
               Returns np.nan if inputs are invalid.
    """
    if not isinstance(k, int) or not isinstance(n, int):
        print("Error: k and n must be integers.")
        return np.nan
    if k < 1 or n < 1:
        print("Error: k and n must be at least 1.")
        return np.nan
    if k > n:
        print(f"Error: Number of used features k ({k}) cannot be greater than total features n ({n}).")
        return np.nan

    if n == 1:
        # If there's only one feature total, using it means maximum interpretability.
        return 1.0
    else:
        interpretability = 1.0 - (k - 1.0) / (n - 1.0)
        # Clamp value between 0 and 1 in case of potential float inaccuracies
        return max(0.0, min(1.0, interpretability))

def calculate_accii(y_true, y_pred, k, n, alpha=0.5):
    """
    Calculates the Accuracy Interpretability Index (AccII).

    AccII balances model accuracy and interpretability.
    AccII = alpha * Accuracy + (1 - alpha) * I

    Args:
        y_true (array-like): Ground truth labels.
        y_pred (array-like): Predicted labels.
        k (int): The number of features used by the final model.
        n (int): The total number of features available before feature selection.
        alpha (float, optional): Weighting factor for accuracy vs. interpretability.
                                 Defaults to 0.5 (equal weight). Must be between 0 and 1.

    Returns:
        float: The AccII score. Returns np.nan if inputs are invalid.
    """
    if not (0 <= alpha <= 1):
        print("Error: alpha must be between 0 and 1.")
        return np.nan

    interpretability = calculate_interpretability(k, n)
    if np.isnan(interpretability):
        return np.nan # Propagate error

    try:
        accuracy = accuracy_score(y_true, y_pred)
    except ValueError as e:
        print(f"Error calculating accuracy: {e}")
        return np.nan

    accii = alpha * accuracy + (1.0 - alpha) * interpretability
    return accii

def calculate_aurocii(y_true, y_scores, k, n, alpha=0.5):
    """
    Calculates the AUROC Interpretability Index (AUROCII).

    AUROCII balances model AUROC and interpretability.
    AUROCII = alpha * AUROC + (1 - alpha) * I

    Args:
        y_true (array-like): Ground truth labels (binary).
        y_scores (array-like): Target scores, can either be probability estimates
                               of the positive class, confidence values, or non-thresholded
                               decision values.
        k (int): The number of features used by the final model.
        n (int): The total number of features available before feature selection.
        alpha (float, optional): Weighting factor for AUROC vs. interpretability.
                                 Defaults to 0.5 (equal weight). Must be between 0 and 1.

    Returns:
        float: The AUROCII score. Returns np.nan if inputs are invalid.
    """
    if not (0 <= alpha <= 1):
        print("Error: alpha must be between 0 and 1.")
        return np.nan

    interpretability = calculate_interpretability(k, n)
    if np.isnan(interpretability):
        return np.nan # Propagate error

    try:
        # Ensure y_true contains only two classes for AUROC calculation
        if len(np.unique(y_true)) > 2:
             print("Warning: AUROC is only defined for binary classification. Treating as multi-class OVR/OVO might give misleading results.")
             # Handle multi-class case if needed, e.g., using roc_auc_score(..., multi_class='ovr')
             # For simplicity, returning NaN for non-binary cases here.
             print("Error: y_true must contain only two unique classes for standard AUROC calculation.")
             return np.nan
        elif len(np.unique(y_true)) < 2:
             print("Error: y_true must contain at least two unique classes for AUROC calculation.")
             return np.nan

        auroc = roc_auc_score(y_true, y_scores)
    except ValueError as e:
        print(f"Error calculating AUROC: {e}")
        return np.nan
    except Exception as e:
        print(f"An unexpected error occurred during AUROC calculation: {e}")
        return np.nan


    aurocii = alpha * auroc + (1.0 - alpha) * interpretability
    return aurocii

# --- Example Usage ---
if __name__ == '__main__':
    # --- Example 1: Simple Binary Classification ---
    print("--- Example 1 ---")
    y_true_binary = np.array([0, 1, 0, 1, 0, 0, 1, 1])
    y_pred_binary = np.array([0, 1, 0, 0, 0, 1, 1, 1]) # Predictions (labels)
    y_scores_binary = np.array([0.1, 0.9, 0.2, 0.4, 0.3, 0.6, 0.8, 0.7]) # Prediction scores/probabilities

    total_features_available = 20 # Example: n
    features_used_by_model = 5   # Example: k
    alpha_weight = 0.5           # Example: alpha

    # Calculate metrics
    I = calculate_interpretability(k=features_used_by_model, n=total_features_available)
    acc = accuracy_score(y_true_binary, y_pred_binary)
    auc = roc_auc_score(y_true_binary, y_scores_binary)
    accii = calculate_accii(y_true_binary, y_pred_binary, k=features_used_by_model, n=total_features_available, alpha=alpha_weight)
    aurocii = calculate_aurocii(y_true_binary, y_scores_binary, k=features_used_by_model, n=total_features_available, alpha=alpha_weight)

    print(f"Total Features (n): {total_features_available}")
    print(f"Features Used (k): {features_used_by_model}")
    print(f"Alpha: {alpha_weight}")
    print(f"Interpretability (I): {I:.4f}")
    print(f"Accuracy: {acc:.4f}")
    print(f"AUROC: {auc:.4f}")
    print(f"AccII: {accii:.4f}")
    print(f"AUROCII: {aurocii:.4f}")
    print("-" * 15)

    # --- Example 2: Edge Case n=1 ---
    print("--- Example 2 (Edge Case n=1) ---")
    I_edge = calculate_interpretability(k=1, n=1)
    accii_edge = calculate_accii(y_true_binary, y_pred_binary, k=1, n=1, alpha=alpha_weight)
    aurocii_edge = calculate_aurocii(y_true_binary, y_scores_binary, k=1, n=1, alpha=alpha_weight)
    print(f"Total Features (n): 1")
    print(f"Features Used (k): 1")
    print(f"Interpretability (I): {I_edge:.4f}")
    print(f"AccII: {accii_edge:.4f}")
    print(f"AUROCII: {aurocii_edge:.4f}")
    print("-" * 15)

    # --- Example 3: Invalid Input k > n ---
    print("--- Example 3 (Invalid Input k > n) ---")
    I_invalid = calculate_interpretability(k=10, n=5)
    print(f"Interpretability (I) for k=10, n=5: {I_invalid}") # Should print error and return nan
    print("-" * 15)

    # --- Example 4: Different Alpha ---
    print("--- Example 4 (Different Alpha) ---")
    alpha_weight_high_perf = 0.8 # Prioritize performance more
    accii_alpha = calculate_accii(y_true_binary, y_pred_binary, k=features_used_by_model, n=total_features_available, alpha=alpha_weight_high_perf)
    aurocii_alpha = calculate_aurocii(y_true_binary, y_scores_binary, k=features_used_by_model, n=total_features_available, alpha=alpha_weight_high_perf)
    print(f"Total Features (n): {total_features_available}")
    print(f"Features Used (k): {features_used_by_model}")
    print(f"Alpha: {alpha_weight_high_perf}")
    print(f"AccII (alpha={alpha_weight_high_perf}): {accii_alpha:.4f}")
    print(f"AUROCII (alpha={alpha_weight_high_perf}): {aurocii_alpha:.4f}")
    print("-" * 15)